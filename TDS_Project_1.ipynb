{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import pandas as pd\n",
        "import time\n",
        "import logging\n",
        "from typing import List, Dict, Any\n",
        "\n",
        "class GitHubScraper:\n",
        "    def __init__(self, token: str):\n",
        "        \"\"\"\n",
        "        Initialize the GitHub scraper with your API token.\n",
        "\n",
        "        Args:\n",
        "            token (str): GitHub Personal Access Token\n",
        "        \"\"\"\n",
        "        self.headers = {\n",
        "            'Authorization': f'token {token}',\n",
        "            'Accept': 'application/vnd.github.v3+json'\n",
        "        }\n",
        "        self.base_url = 'https://api.github.com'\n",
        "\n",
        "        # Setup logging\n",
        "        logging.basicConfig(\n",
        "            level=logging.INFO,\n",
        "            format='%(asctime)s - %(levelname)s - %(message)s'\n",
        "        )\n",
        "        self.logger = logging.getLogger(__name__)\n",
        "\n",
        "    def _make_request(self, url: str, params: dict = None) -> Dict:\n",
        "        \"\"\"\n",
        "        Make a request to the GitHub API with rate limit handling.\n",
        "        \"\"\"\n",
        "        while True:\n",
        "            response = requests.get(url, headers=self.headers, params=params)\n",
        "\n",
        "            if response.status_code == 200:\n",
        "                return response.json()\n",
        "            elif response.status_code == 403:\n",
        "                reset_time = int(response.headers.get('X-RateLimit-Reset', 0))\n",
        "                sleep_time = max(reset_time - time.time(), 0) + 1\n",
        "                self.logger.warning(f\"Rate limit hit. Sleeping for {sleep_time} seconds\")\n",
        "                time.sleep(sleep_time)\n",
        "            else:\n",
        "                self.logger.error(f\"Error {response.status_code}: {response.text}\")\n",
        "                response.raise_for_status()\n",
        "\n",
        "    def clean_company_name(self, company: str) -> str:\n",
        "        \"\"\"\n",
        "        Clean up company names according to specifications.\n",
        "        \"\"\"\n",
        "        if not company:\n",
        "            return \"\"\n",
        "\n",
        "        # Strip whitespace and @ symbol\n",
        "        cleaned = company.strip().lstrip('@')\n",
        "\n",
        "        # Convert to uppercase\n",
        "        return cleaned.upper()\n",
        "\n",
        "    def search_users(self, location: str, min_followers: int) -> List[Dict]:\n",
        "        \"\"\"\n",
        "        Search for GitHub users in a specific location with minimum followers.\n",
        "        \"\"\"\n",
        "        users = []\n",
        "        page = 1\n",
        "\n",
        "        while True:\n",
        "            self.logger.info(f\"Fetching users page {page}\")\n",
        "\n",
        "            query = f\"location:{location} followers:>={min_followers}\"\n",
        "            params = {\n",
        "                'q': query,\n",
        "                'per_page': 100,\n",
        "                'page': page\n",
        "            }\n",
        "\n",
        "            url = f\"{self.base_url}/search/users\"\n",
        "            response = self._make_request(url, params)\n",
        "\n",
        "            if not response['items']:\n",
        "                break\n",
        "\n",
        "            for user in response['items']:\n",
        "                user_data = self._make_request(user['url'])\n",
        "\n",
        "                # Extract only the required fields with exact matching names\n",
        "                cleaned_data = {\n",
        "                    'login': user_data['login'],\n",
        "                    'name': user_data['name'] if user_data['name'] else \"\",\n",
        "                    'company': self.clean_company_name(user_data.get('company')),\n",
        "                    'location': user_data['location'] if user_data['location'] else \"\",\n",
        "                    'email': user_data['email'] if user_data['email'] else \"\",\n",
        "                    'hireable': user_data['hireable'] if user_data['hireable'] is not None else False,\n",
        "                    'bio': user_data['bio'] if user_data['bio'] else \"\",\n",
        "                    'public_repos': user_data['public_repos'],\n",
        "                    'followers': user_data['followers'],\n",
        "                    'following': user_data['following'],\n",
        "                    'created_at': user_data['created_at']\n",
        "                }\n",
        "\n",
        "                users.append(cleaned_data)\n",
        "\n",
        "            page += 1\n",
        "\n",
        "        return users\n",
        "\n",
        "    def get_user_repositories(self, username: str, max_repos: int = 500) -> List[Dict]:\n",
        "        \"\"\"\n",
        "        Get repositories for a specific user.\n",
        "        \"\"\"\n",
        "        repos = []\n",
        "        page = 1\n",
        "\n",
        "        while len(repos) < max_repos:\n",
        "            self.logger.info(f\"Fetching repositories for {username}, page {page}\")\n",
        "\n",
        "            params = {\n",
        "                'sort': 'pushed',\n",
        "                'direction': 'desc',\n",
        "                'per_page': 100,\n",
        "                'page': page\n",
        "            }\n",
        "\n",
        "            url = f\"{self.base_url}/users/{username}/repos\"\n",
        "            response = self._make_request(url, params)\n",
        "\n",
        "            if not response:\n",
        "                break\n",
        "\n",
        "            for repo in response:\n",
        "                # Extract only the required fields with exact matching names\n",
        "                repo_data = {\n",
        "                    'login': username,  # Adding owner's login as required\n",
        "                    'full_name': repo['full_name'],\n",
        "                    'created_at': repo['created_at'],\n",
        "                    'stargazers_count': repo['stargazers_count'],\n",
        "                    'watchers_count': repo['watchers_count'],\n",
        "                    'language': repo['language'] if repo['language'] else \"\",\n",
        "                    'has_projects': repo['has_projects'],\n",
        "                    'has_wiki': repo['has_wiki'],\n",
        "                    'license_name': repo['license']['key'] if repo.get('license') else \"\"\n",
        "                }\n",
        "\n",
        "                repos.append(repo_data)\n",
        "\n",
        "            if len(response) < 100:\n",
        "                break\n",
        "\n",
        "            page += 1\n",
        "\n",
        "        return repos[:max_repos]\n",
        "\n",
        "def main():\n",
        "    # Get GitHub token\n",
        "    token = input(\"Enter your GitHub token: \").strip()\n",
        "    if not token:\n",
        "        print(\"Token is required. Exiting...\")\n",
        "        return\n",
        "\n",
        "    # Initialize scraper\n",
        "    scraper = GitHubScraper(token)\n",
        "\n",
        "    # Search for users in Seattle with > 200 followers\n",
        "    users = scraper.search_users(location='Seattle', min_followers=200)\n",
        "\n",
        "    # Save users to CSV\n",
        "    users_df = pd.DataFrame(users)\n",
        "    users_df.to_csv('users.csv', index=False)\n",
        "\n",
        "    # Get repositories for each user\n",
        "    all_repos = []\n",
        "    for user in users:\n",
        "        repos = scraper.get_user_repositories(user['login'])\n",
        "        all_repos.extend(repos)\n",
        "\n",
        "    # Save repositories to CSV\n",
        "    repos_df = pd.DataFrame(all_repos)\n",
        "    repos_df.to_csv('repositories.csv', index=False)\n",
        "\n",
        "    print(f\"Scraped {len(users)} users and {len(all_repos)} repositories\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "id": "Y7kmliuaFHbx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "\n",
        "# Define the list to store users from Delhi\n",
        "users_in_seattle = []\n",
        "\n",
        "# Read the CSV file with UTF-8 encoding\n",
        "with open('users.csv', 'r', encoding='utf-8') as file:\n",
        "    reader = csv.DictReader(file)\n",
        "    for row in reader:\n",
        "        location = row['location'].strip().lower()\n",
        "        # Check if the user is from Delhi\n",
        "        if 'seattle' in location:\n",
        "            users_in_seattle.append({\n",
        "                'login': row['login'],\n",
        "                'followers': int(row['followers'])\n",
        "            })\n",
        "\n",
        "# Sort users based on followers in descending order\n",
        "top_users = sorted(users_in_seattle, key=lambda x: x['followers'], reverse=True)\n",
        "\n",
        "# Extract the top 5 user logins\n",
        "top_5_logins = [user['login'] for user in top_users[:5]]\n",
        "\n",
        "# Print the result as a comma-separated list\n",
        "print(','.join(top_5_logins))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7lyU7WN8G5nx",
        "outputId": "12aeb969-b969-4e6e-d852-697d801f78a3"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "vczh,bradfitz,munificent,tenderlove,koush\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert created_at to datetime and sort in ascending order\n",
        "users_df['created_at'] = pd.to_datetime(users_df['created_at'])\n",
        "earliest_5_users = users_df.sort_values(by='created_at').head(5)['login'].tolist()\n",
        "\n",
        "print(','.join(earliest_5_users))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7V9liKDNL3wA",
        "outputId": "b1181287-5080-45fa-91ce-73a0f8fda817"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "topfunky,nex3,beccasaurus,eric,grantr\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "repos_df = pd.read_csv('repositories.csv')\n",
        "\n",
        "# Ignore missing licenses\n",
        "license_counts = repos_df[repos_df['license_name'].notnull()]['license_name'].value_counts().head(3)\n",
        "\n",
        "print(','.join(license_counts.index))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NKSzMrb0MNjX",
        "outputId": "2c1d22d9-1d1d-4bea-b4b8-dd6ac1b2a913"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mit,apache-2.0,other\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Ignore missing or empty company fields\n",
        "company_counts = users_df[users_df['company'].notnull()]['company'].value_counts()\n",
        "\n",
        "# Get the company with the most users\n",
        "majority_company = company_counts.idxmax()\n",
        "\n",
        "print(majority_company)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wo3UmHj_VtXT",
        "outputId": "0338ec11-4478-464d-b6c0-7b1d8bf670c3"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MICROSOFT\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Ignore missing or empty languages\n",
        "language_counts = repos_df[repos_df['language'].notnull()]['language'].value_counts()\n",
        "\n",
        "# Get the most popular language\n",
        "most_popular_language = language_counts.idxmax()\n",
        "\n",
        "print(most_popular_language)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9n_sGVr2VuXd",
        "outputId": "d3a1c23e-dcfe-4556-f4ad-c5234efe1ff4"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "JavaScript\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Filter users who joined after 2020\n",
        "users_after_2020 = users_df[users_df['created_at'] >= '2020-01-01']\n",
        "\n",
        "# Get their logins\n",
        "logins_after_2020 = users_after_2020['login'].tolist()\n",
        "\n",
        "# Filter repositories for these users\n",
        "repos_after_2020 = repos_df[repos_df['login'].isin(logins_after_2020)]\n",
        "\n",
        "# Find the second most popular language\n",
        "language_counts_after_2020 = repos_after_2020['language'].value_counts()\n",
        "\n",
        "second_most_popular_language = language_counts_after_2020.index[1]  # Second most common language\n",
        "\n",
        "print(second_most_popular_language)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mfwgVlEBWN8Z",
        "outputId": "52396266-aa10-40f9-de46-1d3a75ffe808"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Python\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Group by language and calculate average stars\n",
        "avg_stars_by_language = repos_df.groupby('language')['stargazers_count'].mean()\n",
        "\n",
        "# Get the language with the highest average stars\n",
        "language_highest_avg_stars = avg_stars_by_language.idxmax()\n",
        "\n",
        "print(language_highest_avg_stars)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2PXtGMamWRR-",
        "outputId": "3820aae0-2c5a-4e48-8d5a-75417b27fa94"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Haml\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate leader_strength\n",
        "users_df['leader_strength'] = users_df['followers'] / (1 + users_df['following'])\n",
        "\n",
        "# Sort users by leader_strength and get the top 5\n",
        "top_5_leader_strength = users_df.sort_values(by='leader_strength', ascending=False).head(5)['login'].tolist()\n",
        "\n",
        "print(','.join(top_5_leader_strength))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_9jV4N-fWUzT",
        "outputId": "e1d17790-5987-4256-d106-40f5a7eb3a40"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "awslabs,mission-peace,karan,cmuratori,nex3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate correlation between followers and public repos\n",
        "correlation = users_df['followers'].corr(users_df['public_repos'])\n",
        "\n",
        "print(f'{correlation:.3f}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "amBBc8-5WYXI",
        "outputId": "963690a6-09a0-4c53-d09f-7e7143a3da3c"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.200\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LinearRegression\n",
        "import numpy as np\n",
        "\n",
        "# Prepare the data\n",
        "X = users_df['public_repos'].values.reshape(-1, 1)\n",
        "y = users_df['followers'].values\n",
        "\n",
        "# Fit the model\n",
        "reg_model = LinearRegression()\n",
        "reg_model.fit(X, y)\n",
        "\n",
        "# Slope of the regression (followers per public repo)\n",
        "slope = reg_model.coef_[0]\n",
        "\n",
        "print(f'{slope:.3f}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "daTb-01zWcLO",
        "outputId": "112be9e6-20d1-4111-ada7-435fe76dd907"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.435\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate correlation between has_projects and has_wiki\n",
        "projects_wiki_corr = repos_df['has_projects'].corr(repos_df['has_wiki'])\n",
        "\n",
        "print(f'{projects_wiki_corr:.3f}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aKewjdbHWfja",
        "outputId": "123d8a0f-34b6-439f-d0aa-1f9425f43d11"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.311\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "def analyze_following_difference(users_csv_path='users.csv'):\n",
        "    # Read the data\n",
        "    df = pd.read_csv(users_csv_path)\n",
        "\n",
        "    # Calculate average following for hireable users\n",
        "    hireable_following = df[df['hireable'] == True]['following'].mean()\n",
        "\n",
        "    # Calculate average following for non-hireable users\n",
        "    non_hireable_following = df[df['hireable'] != True]['following'].mean()\n",
        "\n",
        "    # Calculate the difference rounded to 3 decimal places\n",
        "    difference = round(hireable_following - non_hireable_following, 3)\n",
        "\n",
        "    # Print debug information\n",
        "    print(f\"Number of hireable users: {len(df[df['hireable'] == True])}\")\n",
        "    print(f\"Number of non-hireable users: {len(df[df['hireable'] != True])}\")\n",
        "    print(f\"Average following for hireable users: {hireable_following:.3f}\")\n",
        "    print(f\"Average following for non-hireable users: {non_hireable_following:.3f}\")\n",
        "\n",
        "    return difference\n",
        "\n",
        "# Calculate the difference\n",
        "result = analyze_following_difference()\n",
        "print(f\"\\nDifference in average following: {result:.3f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "alw5uAUtWjoq",
        "outputId": "5dcaa828-350d-4f45-8a6c-fc0295a7d929"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of hireable users: 117\n",
            "Number of non-hireable users: 405\n",
            "Average following for hireable users: 69.923\n",
            "Average following for non-hireable users: 56.625\n",
            "\n",
            "Difference in average following: 13.298\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate bio length\n",
        "users_df['bio_length'] = users_df['bio'].apply(lambda x: len(str(x)) if pd.notnull(x) else 0)\n",
        "\n",
        "# Correlation between bio length and followers\n",
        "bio_followers_corr = users_df['bio_length'].corr(users_df['followers'])\n",
        "\n",
        "print(f'{bio_followers_corr:.3f}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2nhOf5jrWmoN",
        "outputId": "770e3cd3-8ee4-425b-9733-b40d073ccb48"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of users with bios: 351\n",
            "Bio length range: 8 to 160\n",
            "Followers range: 200 to 17512\n",
            "R-squared: 0.001\n",
            "\n",
            "Regression slope: 1.040\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert created_at to datetime\n",
        "repos_df['created_at'] = pd.to_datetime(repos_df['created_at'])\n",
        "\n",
        "# Filter repositories created on weekends (Saturday or Sunday)\n",
        "repos_on_weekends = repos_df[repos_df['created_at'].dt.weekday >= 5]\n",
        "\n",
        "# Count repositories by user\n",
        "weekend_repo_counts = repos_on_weekends['login'].value_counts().head(5)\n",
        "\n",
        "print(','.join(weekend_repo_counts.index))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hM57kb_jWqwF",
        "outputId": "92018e6c-54c3-4cf7-b8c1-b749b794a5eb"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "nolanlawson,homebysix,ingydotnet,anvaka,moznion\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "def analyze_email_sharing(users_csv_path='users.csv'):\n",
        "    # Read the complete CSV file\n",
        "    df = pd.read_csv(users_csv_path)\n",
        "\n",
        "    # Convert email column to boolean (True if email exists, False if NaN or empty)\n",
        "    df['has_email'] = df['email'].notna() & (df['email'] != '')\n",
        "\n",
        "    # Calculate for hireable users\n",
        "    hireable_mask = df['hireable'] == True\n",
        "    if hireable_mask.any():\n",
        "        hireable_email_fraction = df[hireable_mask]['has_email'].mean()\n",
        "    else:\n",
        "        hireable_email_fraction = 0\n",
        "\n",
        "    # Calculate for non-hireable users\n",
        "    non_hireable_mask = df['hireable'] != True\n",
        "    if non_hireable_mask.any():\n",
        "        non_hireable_email_fraction = df[non_hireable_mask]['has_email'].mean()\n",
        "    else:\n",
        "        non_hireable_email_fraction = 0\n",
        "\n",
        "    # Calculate difference and round to 3 decimal places\n",
        "    difference = round(hireable_email_fraction - non_hireable_email_fraction, 3)\n",
        "\n",
        "    # Print debug information\n",
        "    print(f\"Total users: {len(df)}\")\n",
        "    print(f\"Hireable users with email: {df[hireable_mask]['has_email'].sum()}/{hireable_mask.sum()}\")\n",
        "    print(f\"Non-hireable users with email: {df[non_hireable_mask]['has_email'].sum()}/{non_hireable_mask.sum()}\")\n",
        "    print(f\"Hireable fraction: {hireable_email_fraction:.3f}\")\n",
        "    print(f\"Non-hireable fraction: {non_hireable_email_fraction:.3f}\")\n",
        "\n",
        "    return difference\n",
        "\n",
        "# Read and analyze the complete dataset\n",
        "result = analyze_email_sharing()\n",
        "print(f\"\\nFinal result: {result:.3f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O7o2bc4dWuOQ",
        "outputId": "f0ba80c6-66d6-4c2f-ccb8-a67935d2020f"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total users: 522\n",
            "Hireable users with email: 71/117\n",
            "Non-hireable users with email: 210/405\n",
            "Hireable fraction: 0.607\n",
            "Non-hireable fraction: 0.519\n",
            "\n",
            "Final result: 0.088\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "from collections import Counter\n",
        "\n",
        "# Counter to store surname frequencies\n",
        "surname_counter = Counter()\n",
        "\n",
        "# Open the users.csv file and read data\n",
        "with open('users.csv', 'r', encoding='utf-8') as file:\n",
        "    reader = csv.DictReader(file)\n",
        "\n",
        "    for row in reader:\n",
        "        name = row.get('name', '').strip()\n",
        "        if name:  # Ignore missing names\n",
        "            # Split the name by whitespace and get the last word as the surname\n",
        "            surname = name.split()[-1]\n",
        "            surname_counter[surname] += 1\n",
        "\n",
        "# Find the maximum frequency of surnames\n",
        "if surname_counter:\n",
        "    max_count = max(surname_counter.values())\n",
        "    # Get all surnames with the maximum frequency\n",
        "    most_common_surnames = [surname for surname, count in surname_counter.items() if count == max_count]\n",
        "    # Sort surnames alphabetically\n",
        "    most_common_surnames.sort()\n",
        "    # Output the result\n",
        "    print(f\"{', '.join(most_common_surnames)}: {max_count}\")\n",
        "else:\n",
        "    print(\"No names found.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gu-tOg-0WzhW",
        "outputId": "14d0880c-b4d5-4c9d-ed32-e5b02f356e89"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Wang: 6\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ufGA4nkmW3ZU"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}